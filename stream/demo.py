import streamlit as st
from openai import OpenAI

st.set_page_config(page_title="åƒé—®Plus", layout="wide")
st.title("ğŸ¤– é€šä¹‰åƒé—®Plus")

client = OpenAI(
    api_key="sk-139a40229c0e4bd58191a7a2f8c9c8f3",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

if "messages" not in st.session_state:
    st.session_state.messages = []

def get_completion(text):
    try:
        completion = client.chat.completions.create(
            model="qwen-plus",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                *st.session_state.messages,  # åŒ…å«å†å²æ¶ˆæ¯
                {"role": "user", "content": text},
            ],
            stream=True,
            stream_options={"include_usage": True}
        )
        return completion
    except Exception as e:
        st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        return None

# æ˜¾ç¤ºèŠå¤©å†å²ï¼Œæœ¬è´¨æ˜¯å¾ªç¯å†æ˜¾ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è·å–ç”¨æˆ·è¾“å…¥,æµ·è±¡è¿ç®—ç¬¦ := ä¸€ç§èµ‹å€¼è¡¨è¾¾å¼ï¼Œå…è®¸åœ¨è¡¨è¾¾å¼ä¸­åŒæ—¶è¿›è¡Œèµ‹å€¼å’Œæ¡ä»¶åˆ¤æ–­
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ä½¿ç”¨ st.chat_message("user") åˆ›å»ºä¸€ä¸ªä»£è¡¨ç”¨æˆ·çš„æ¶ˆæ¯æ°”æ³¡,åœ¨æ¶ˆæ¯æ°”æ³¡å†…æ˜¾ç¤ºç”¨æˆ·è¾“å…¥çš„å†…å®¹
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # ä½¿ç”¨ st.empty() åˆ›å»ºä¸€ä¸ªç©ºå®¹å™¨ï¼Œç”¨äºåŠ¨æ€æ›´æ–°å†…å®¹ï¼Œåˆ›å»º full_response å­—ç¬¦ä¸²ï¼Œç”¨äºé€æ­¥æ„å»ºå®Œæ•´çš„ AI å“åº”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # å¤„ç†æµå¼å“åº”
        response = get_completion(prompt)
        if response:
            try:
                for chunk in response:
                    if hasattr(chunk, 'choices') and chunk.choices and hasattr(chunk.choices[0], 'delta') and hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "â–Œ")# ä½¿ç”¨â–Œå­—ç¬¦æ¨¡æ‹Ÿæ‰“å­—å…‰æ ‡ï¼Œå¢å¼ºäº¤äº’æ„Ÿã€‚
                
                message_placeholder.markdown(full_response)
                
                # åªæœ‰åœ¨æˆåŠŸè·å–åˆ°å“åº”æ—¶æ‰æ·»åŠ åˆ°å†å²è®°å½•
                if full_response:
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                else:
                    st.error("æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„å“åº”")
            except Exception as e:
                st.error(f"å¤„ç†å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


if st.sidebar.button("æ¸…é™¤èŠå¤©å†å²"):
    st.session_state.messages = []
    st.rerun()  # é‡æ–°è¿è¡Œåº”ç”¨ï¼Œä½¿ç•Œé¢æ›´æ–°åæ˜ æ¸…é™¤åçš„çŠ¶æ€


with st.sidebar:
    st.markdown("""
    ### ä½¿ç”¨è¯´æ˜
    1. åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜
    2. æŒ‰å›è½¦é”®æˆ–ç‚¹å‡»å‘é€æŒ‰é’®
    3. ç­‰å¾…AIåŠ©æ‰‹çš„å›ç­”
    4. å¯ä»¥ç‚¹å‡»"æ¸…é™¤èŠå¤©å†å²"é‡æ–°å¼€å§‹å¯¹è¯
    
    ### åŠŸèƒ½ç‰¹ç‚¹
    - æ”¯æŒæµå¼è¾“å‡º
    - ä¿å­˜èŠå¤©å†å²
    - æ”¯æŒMarkdownæ ¼å¼
    """)